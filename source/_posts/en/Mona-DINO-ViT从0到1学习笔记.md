---
title: Mona-DINO-ViTä»0åˆ°1å­¦ä¹ ç¬”è®°
catalog: true
date: 2025-10-19 10:46:51
subtitle: æ¨¡å‹å¾®è°ƒå…¥é—¨ç¯‡
sticky: 23
lang: en
header-img: /img/header_img/VSCappearance.png
tags:
- [MedicalImage]
categories:
- [MedicalImage]
---

## Foreword

â€‹	åŒ—äº¬å·²ç»å¿«å…¥å†¬äº†ã€‚ç¬”è€…ä»Šæ—©å·®ç‚¹å†»æ­»åœ¨è‡ªè¡Œè½¦ä¸Šã€‚

â€‹	æœ€è¿‘äº‹æƒ…å¾ˆå¤šå¾ˆå¤šï¼Œä½†æ˜¯æƒ³äº†ä¸€æƒ³è¿˜æ˜¯è§‰å¾—éœ€è¦å†™ä¸€ç¯‡åšå®¢è®°å½•è¿™æ®µä¸å‡¡çš„æ—…ç¨‹ã€‚å¦‚è‹¥æ²¡æœ‰ä¸€ä¸ªç¨³å®šçš„è®°å½•ï¼Œææ€•è¿‡å‡ å¤©ä¹Ÿä¼šè¢«ç¬¨äººé—å¿˜ï¼Œæ•…ä¸‹å®šå†³å¿ƒä»¥è®°ä¹‹ã€‚

â€‹	æ­¤å¤–ï¼Œç¬”è€…å¿™é‡Œå·é—²å®‰ç½®äº†ä¸€ä¸ªå­åŸŸåç½‘ç«™åšéšç¬”æ—¥è®°æœ¬ï¼Œä½†æ˜¯cloudflareé…ç½®æœ€è¿‘å‡ºäº†ç‚¹é—®é¢˜ï¼Œæ„Ÿå…´è¶£çš„æœ‹å‹å¯ä»¥å…ˆæœç´¢`http://101.200.30.9:6277/`~~ï¼ˆæ”¾åœ¨è¿™é‡Œä¼šä¸ä¼šä¸å¤ªå®‰å…¨ï¼‰~~ï¼Œåç»­å°†ä»¥`echo.0mnilink.top`çš„å½¢æ€å’Œå¤§å®¶è§é¢ï¼Œç¬”è€…ä¼šåœ¨é‡Œé¢å®šæœŸçœŸå®å‘ç–¯ï¼Œæ›´æŠ½è±¡ã€æ›´è´´è¿‘ç”Ÿæ´»ğŸ˜®

## Beginning of the story

â€‹	æˆ‘ä»¬çš„æ ¸å¿ƒç›®æ ‡å°±æ˜¯å­¦ä¹ ã€äº†è§£adaptorå¾®è°ƒæ¨¡å‹çš„éƒ¨åˆ†ç»†èŠ‚ä¸é¡¹ç›®æ¡†æ¶ï¼Œå¹¶ä¸”å’Œè°ƒç ”éœ€æ±‚ç´§å¯†ç»“åˆï¼Œæœ€åç»™å‡ºä¸€ä¸ªæ¯”è¾ƒreasonableçš„resultï¼Œæ€è·¯æµç¨‹å¦‚å›¾æ‰€ç¤ºï¼š
![pipeline](pipeline.png)

â€‹	è¿™ç¯‡åšå®¢å°†ä½œä¸ºè®°å½•å‹åšå®¢æŒç»­æ›´æ–°ï¼Œä¼´éšç¬”è€…å®Œæˆæ¯ä¸€æ­¥çš„ä»»åŠ¡ï¼äº‹ä¸å®œè¿Ÿï¼Œå‡†å¤‡å‡ºå‘ï¼

## Understand & Prepare

### ç†è§£Monaçš„æ ¸å¿ƒæœºåˆ¶

â€‹	æˆ‘ä»¬æ ¹æ®ç»éªŒï¼Œä¸ºè‡ªå·±å‡†å¤‡äº†ä¸‰ä¸ªä¸»çº¿ä»»åŠ¡ï¼š

- **è¾“å…¥/è¾“å‡ºï¼š** ææ¸…æ¥šMonaæ¨¡å—çš„forwardå‡½æ•°æ¥æ”¶å“ªäº›è¾“å…¥ï¼Œè¾“å‡ºä»€ä¹ˆã€‚è®°ä¸‹å®ƒä»¬çš„ç»´åº¦ä¿¡æ¯ï¼ˆå¦‚ [Batch, Num_Heads, Seq_Len, Head_Dim]ï¼‰ã€‚
- **ä½œç”¨ä½ç½®ï¼š** æ˜ç™½Monaæ˜¯åœ¨Transformerçš„å“ªä¸ªå…·ä½“è®¡ç®—æ­¥éª¤ä¸­èµ·ä½œç”¨çš„ã€‚æ˜¯åœ¨Q, K, VæŠ•å½±ä¹‹åï¼ŒAttentionåˆ†æ•°è®¡ç®—ä¹‹å‰å—ï¼Ÿè¿˜æ˜¯ä½œç”¨äºFFNå±‚ï¼Ÿã€‚
- **æ ¸å¿ƒå‚æ•°ï¼š** çŸ¥é“rank (ç§©) æ˜¯æ§åˆ¶Monaå‚æ•°é‡çš„å…³é”®è¶…å‚æ•°ã€‚ç†è§£rankè¶Šå¤§ï¼Œå¼•å…¥çš„å‚æ•°è¶Šå¤šï¼Œæ‹Ÿåˆèƒ½åŠ›å¯èƒ½è¶Šå¼ºï¼Œä½†è®­ç»ƒæˆæœ¬ä¹Ÿè¶Šé«˜ã€‚

#### Input/Output of Mona module

```python
class MonaOp(nn.Module):
    def __init__(self, in_features):
        super().__init__()
        self.conv1 = nn.Conv2d(in_features, in_features, kernel_size=3, padding=3 // 2, groups=in_features)
        self.conv2 = nn.Conv2d(in_features, in_features, kernel_size=5, padding=5 // 2, groups=in_features)
        self.conv3 = nn.Conv2d(in_features, in_features, kernel_size=7, padding=7 // 2, groups=in_features)
		# ä¸‰ä¸ªå¹¶è¡Œçš„ æ·±åº¦å¯åˆ†ç¦»å·ç§¯
        self.projector = nn.Conv2d(in_features, in_features, kernel_size=1, )
		# ç”¨äºç‰¹å¾èåˆ
    def forward(self, x):
        identity = x # ç”¨äºç¬¬ä¸€ä¸ªæ®‹å·®è¿æ¥
        conv1_x = self.conv1(x)
        conv2_x = self.conv2(x)
        conv3_x = self.conv3(x)

        x = (conv1_x + conv2_x + conv3_x) / 3.0 + identity

        identity = x # ç”¨äºç¬¬äºŒä¸ªæ®‹å·®è¿æ¥

        x = self.projector(x)

        return identity + x

class Mona(BaseModule):
    def __init__(self,
                 in_dim,
                 factor=4):
        super().__init__()

        self.project1 = nn.Linear(in_dim, 64) # é™ç»´è‡³64
        self.nonlinear = F.gelu
        self.project2 = nn.Linear(64, in_dim) # ä»64å‡ç»´

        self.dropout = nn.Dropout(p=0.1)

        self.adapter_conv = MonaOp(64)

        self.norm = nn.LayerNorm(in_dim)
        self.gamma = nn.Parameter(torch.ones(in_dim) * 1e-6)
        self.gammax = nn.Parameter(torch.ones(in_dim))

    def forward(self, x, hw_shapes=None):
        identity = x

        x = self.norm(x) * self.gamma + x * self.gammax

        project1 = self.project1(x)

        b, n, c = project1.shape
        h, w = hw_shapes
        project1 = project1.reshape(b, h, w, c).permute(0, 3, 1, 2)
        # åºåˆ—[B, L, 64] ==> å›¾åƒç½‘æ ¼ [B, 64, H, W]ï¼Œè¦ç”¨åˆ°å‚æ•°hw_shapes
        project1 = self.adapter_conv(project1)
        project1 = project1.permute(0, 2, 3, 1).reshape(b, n, c)

        nonlinear = self.nonlinear(project1)
        nonlinear = self.dropout(nonlinear)
        project2 = self.project2(nonlinear)

        return identity + project2
```

Mona æ¨¡å—çš„ forward å‡½æ•°æ¥æ”¶ **ä¸¤ä¸ª** inputï¼š

1. **x**: è¿™æ˜¯ä¸»è¦çš„ç‰¹å¾å¼ é‡ï¼Œæ˜¯ä¸Šä¸€å±‚ï¼ˆMSAæˆ–MLPï¼‰çš„è¾“å‡ºã€‚
   - **ç»´åº¦**: [B, L, C]
     - B: Batch Size
     - L: Sequence Length  *= H \* W*
     - C: Channel / Embedding Dimension (ç‰¹å¾ç»´åº¦)
2. **hw_shapes**: è¿™æ˜¯ä¸€ä¸ªåŒ…å«ç‰¹å¾å›¾é«˜åº¦å’Œå®½åº¦çš„å…ƒç»„ (tuple)ã€‚
   - **ä½œç”¨**: è¿™ä¸ªå‚æ•°è‡³å…³é‡è¦ï¼Œå› ä¸º Mona æ¨¡å—å†…éƒ¨éœ€è¦è¿›è¡Œå·ç§¯æ“ä½œã€‚å®ƒå¿…é¡»çŸ¥é“å¦‚ä½•å°†é•¿åº¦ä¸º L çš„åºåˆ— x é‡æ–°å¡‘å½¢ (reshape) å› [H, W] çš„äºŒç»´ç©ºé—´ç½‘æ ¼ã€‚

Mona æ¨¡å—çš„ forward å‡½æ•°è¾“å‡º **ä¸€ä¸ª** outputï¼š

1. **identity + project2**:
   - **ç»´åº¦**: [B, L, C]
     - è¾“å‡ºçš„ç»´åº¦ä¸è¾“å…¥çš„ç‰¹å¾å¼ é‡ x **å®Œå…¨ç›¸åŒ**ã€‚è¿™æ˜¯ä¸ºäº†ç¡®ä¿å®ƒå¯ä»¥æ— ç¼åœ°æ¥å…¥Transformerçš„æ®‹å·®è¿æ¥ç»“æ„ä¸­ã€‚

#### Action Position

```python
class SwinTransformerBlock(nn.Module):
    """ Swin Transformer Block.

    Args:
        dim (int): Number of input channels.
        num_heads (int): Number of attention heads.
        window_size (int): Window size.
        shift_size (int): Shift size for SW-MSA.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.
        drop (float, optional): Dropout rate. Default: 0.0
        attn_drop (float, optional): Attention dropout rate. Default: 0.0
        drop_path (float, optional): Stochastic depth rate. Default: 0.0
        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
    """

    def __init__(self, dim, num_heads, window_size=7, shift_size=0,
                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,
                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):
        super().__init__()
        self.dim = dim
        self.num_heads = num_heads
        self.window_size = window_size
        self.shift_size = shift_size
        self.mlp_ratio = mlp_ratio
        assert 0 <= self.shift_size < self.window_size, "shift_size must in 0-window_size"

        self.norm1 = norm_layer(dim)
        self.attn = WindowAttention(
            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,
            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)

        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)

        self.H = None
        self.W = None

        self.my_module_1 = Mona(dim, 8)
        self.my_module_2 = Mona(dim, 8) # Adapter_FFN(dim, 8)

    def forward(self, x, mask_matrix):
        """ Forward function.

        Args:
            x: Input feature, tensor size (B, H*W, C).
            H, W: Spatial resolution of the input feature.
            mask_matrix: Attention mask for cyclic shift.
        """
        B, L, C = x.shape
        H, W = self.H, self.W
        assert L == H * W, "input feature has wrong size"

        shortcut = x
        x = self.norm1(x)
        x = x.view(B, H, W, C)

        # pad feature maps to multiples of window size
        pad_l = pad_t = 0
        pad_r = (self.window_size - W % self.window_size) % self.window_size
        pad_b = (self.window_size - H % self.window_size) % self.window_size
        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))
        _, Hp, Wp, _ = x.shape

        # cyclic shift
        if self.shift_size > 0:
            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))
            attn_mask = mask_matrix
        else:
            shifted_x = x
            attn_mask = None

        # partition windows
        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C
        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C

        # W-MSA/SW-MSA
        attn_windows = self.attn(x_windows, mask=attn_mask)  # nW*B, window_size*window_size, C

        # merge windows
        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)
        shifted_x = window_reverse(attn_windows, self.window_size, Hp, Wp)  # B H' W' C

        # reverse cyclic shift
        if self.shift_size > 0:
            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))
        else:
            x = shifted_x

        if pad_r > 0 or pad_b > 0:
            x = x[:, :H, :W, :].contiguous()

        x = x.view(B, H * W, C)

        # my_adapter
        # x = self.my_module_1(x)

        x = shortcut + self.drop_path(x)

        x = self.my_module_1(x,(H,W))

        identity = x
        x = self.norm2(x)

        # my_adapter2
        # x = self.my_module_2(x)

        # FFN
        x = self.mlp(x)

        # x = self.my_module_2(x)  # todo: correct

        x = identity + self.drop_path(x)

        x = self.my_module_2(x,(H,W))  # todo: correct

        return x
```

`SwinTransformerBlock`çš„`forward`å‡½æ•°ä¸­è°ƒç”¨ä¸¤æ¬¡`Mona`æ¨¡å—ï¼š

1. `self.my_module1`:

   ä½äºMSA(Multi-Self-Attention)ä¹‹åï¼Œæ¥æ”¶ç»è¿‡MSAå’Œå…¶æ®‹å·®è¿æ¥çš„ç‰¹å¾ï¼Œå¹¶å¯¹ä¹‹è¿›è¡Œé€‚é…ã€‚

2. `self.my_module2`:

   ä½äºMLPä¹‹åï¼Œæ¥æ”¶ç»è¿‡MLPå’Œå…¶æ®‹å·®è¿æ¥çš„ç‰¹å¾ï¼Œä¹ŸåŒæ ·è¿›è¡Œé€‚é…ã€‚

å› æ­¤`Mona`æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„åå¤„ç†æ¨¡å—ï¼Œè¢«æ”¾ç½®åœ¨ Transformer Block çš„ä¸¤ä¸ªæ ¸å¿ƒè®¡ç®—å•å…ƒï¼ˆMSA å’Œ MLPï¼‰çš„è¾“å‡ºä½ç½®ï¼Œå¯¹å®ƒä»¬çš„å¤„ç†ç»“æœè¿›è¡Œå¢å¼ºå’Œé€‚é…ã€‚

#### Core Parameters

```python
self.project1 = nn.Linear(in_dim, 64) # Down-projection to n=64
self.project2 = nn.Linear(64, in_dim) # Up-projection from n=64
self.adapter_conv = MonaOp(64)       # Convolutions operate on n=64 channels
```

**æ ¸å¿ƒè¶…å‚æ•°**: **ä¸­é—´ç»´åº¦ *n***

â€‹	*n* çš„å¤§å°ç›´æ¥æ§åˆ¶äº† Mona æ¨¡å—çš„å‚æ•°é‡å’Œè®¡ç®—å¤æ‚åº¦ã€‚*n* è¶Šå¤§ï¼Œé™ç»´åçš„ä¿¡æ¯æŸå¤±è¶Šå°‘ï¼Œæ‹Ÿåˆèƒ½åŠ›å¯èƒ½è¶Šå¼ºï¼Œä½†åŒæ—¶å¼•å…¥çš„å‚æ•°ä¹Ÿè¶Šå¤šã€‚

